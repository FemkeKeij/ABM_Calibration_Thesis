---
title: "Fire_00_Run_Model"
author: "Femke Keij S2647168"
date: "2023-02-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import Python modules:
```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
import itertools
import statistics
sns.set_style('white')
sns.set_context('talk')

import pyNetLogo # to run NetLogo from RStudio
```

Import R packages:
```{r}
# for importing / working with tidy data
library(tidyverse)
# for arranging plots
library(patchwork)
# for cumulative computations
library(zoo)
# for rolling averages
library(slider)
# for latin hypercube sampling
library(lhs)
```

Set seed:
```{r}
set.seed(42)
```

Link to and start the Fire model:
```{python}
netlogo = pyNetLogo.NetLogoLink(gui = True) # start netlogo

netlogo.load_model(r"C:\Users\Femke Keij\OneDrive\Thesis\R projects\LU thesis GitHub\models\Fire_myversion.nlogo")
# open model
```

# Decide on model outputs
I'll use the burn percentage at each tick. There is no set number of ticks in this case, since the simulation stops when the burn does.

# Determine number of runs
1) coefficient of variation
2) mean convergence
3) windowed variance

Optionally, we can set a separate convergence boundary $\epsilon$ for each output. This is only necessary when the outputs are on different scales, which is not the case here. I choose $\epsilon_{c_\upsilon} = 0.1$, $\epsilon_\mu = 1$, and $\epsilon_{\omega^2} = 0.2$.

We can compute all these statistics using just 1 data set. To do so, we run each parameter combination 50 times and save the outputs at the last time step. I combine a low, mid, and high value for tree density with both combinations for directions.
```{python}
dictionary = {'density': [5, 50, 95], 'directions': [4, 8]}

def expand_grid(data_dict):
    rows = itertools.product(*data_dict.values())
    return pd.DataFrame.from_records(rows, columns = data_dict.keys())

param_comb = expand_grid(dictionary)
```

Obtain all necessary data:
```{python}
# data frame to store results
out_df = pd.DataFrame(columns = ['density', 'directions', 'run', 'burn_percentage', 'tick'])
# add empty row to start
out_df = out_df.append({'density': 0, 'directions': 0, 'run': 0, 'burn_percentage': -100, 'tick': - 100}, ignore_index = True)

# set counter
count = 0
# set random seed
netlogo.command('clean')

for i in range(0,len(param_comb)):
  # set parameters
  density = str(param_comb['density'][i])
  directions = str(param_comb['directions'][i])
  netlogo.command(''.join(['set density ', density]))
  netlogo.command(''.join(['set directions ', directions]))
  
  # repeat 50 times with set parameters
  for x in range(0, 50):
    # setup
    netlogo.command('setup')
    # run
    netlogo.command('repeat 10000 [go]')
    # record number of burned trees and number of initial trees
    initial = netlogo.report('initial-trees')
    burned = netlogo.report('burned-trees')
    # calculate % burned
    burn_percentage = burned / initial * 100
    # ticks before end of simulation
    ticks = netlogo.report('ticks')
    # store
    out_df = out_df.append({'density': density, 'directions': directions, 'run': x + 1, 'burn_percentage': burn_percentage, 'tick': ticks}, ignore_index = True)
    
out_df.to_csv('data/raw/fire_numruns.csv')
```

Compute statistics:
```{r}
numruns <- read.csv('data/raw/fire_numruns.csv')

# remove first row & column
numruns <- numruns %>%
  slice(-1) %>%
  select(-1)

# add a unique number for each parameter combination
numruns <- numruns %>%
  group_by(density, directions) %>%
  mutate(combination_number = cur_group_id())
```

1) Mean convergence
With $\epsilon_\mu = 1$
```{r}
numruns <- numruns %>%
  group_by(combination_number, tick) %>%
  # compute cumulative mean
  # and differences between means
  mutate(mean_burn = cummean(burn_percentage),
         mean_diff_burn = mean_burn - 
           lag(mean_burn, default = mean_burn[1]))

# compute at which point the mean difference settles below 1
mean_conv_burn <- numruns %>%
  group_by(combination_number) %>%
  filter(abs(mean_diff_burn) < 1,
         run != 1) %>%
  slice(which.min(run))

# plot
p1 <- numruns %>%
  ggplot(mapping = aes(x = run, y = mean_diff_burn,
                       group = combination_number)) +
  geom_line(colour = 'lightgrey') +
  geom_hline(yintercept = 1, colour = 'darkgreen') +
  geom_hline(yintercept = - 1, colour = 'darkgreen') +
  theme_minimal() +
  labs(x = 'runs added',
       y = 'difference burn percentage')

p2 <- mean_conv_burn %>%
  ggplot(mapping = aes(x = run)) +
  geom_histogram(binwidth = 1) +
  theme_minimal() +
  labs(y = 'count',
       x = 'number of runs until mean convergence')

# plot both
mean_conv_plot <- p1 / p2
```

2) Coefficient of variation
With $\epsilon_{c_\upsilon} = 0.1$.
```{r}
numruns <- numruns %>%
  group_by(combination_number) %>%
  # compute cumulative standard deviations
  # compute coefficient of variation
  mutate(sd_burn = rollapplyr(burn_percentage, 1:n(), sd),
         cv_burn = sd_burn / mean_burn) %>%
  # replace all NAs (resulting from sd = 0) with 0
  replace(is.na(.), 0) %>%
  # compute differences in cv
  mutate(cv_diff = cv_burn - lag(cv_burn, default = cv_burn[1]))
  
# compute at which point the mean difference settles below 1
cv_conv_burn <- numruns %>%
  group_by(combination_number) %>%
  filter(abs(cv_diff) < 0.1,
         run != 1) %>%
  slice(which.min(run))

# plot
p1 <- numruns %>%
  ggplot(mapping = aes(x = run, y = cv_diff,
                       group = combination_number)) +
  geom_line(colour = 'lightgrey') +
  geom_hline(yintercept = 0.1, colour = 'darkgreen') +
  geom_hline(yintercept = - 0.1, colour = 'darkgreen') +
  theme_minimal() +
  labs(x = 'added runs',
       y = 'cv burn percentage')

p2 <- cv_conv_burn %>%
  ggplot(mapping = aes(x = run)) +
  geom_histogram(binwidth = 1) +
  geom_vline(xintercept = mean(cv_conv_burn$run),
             colour = 'darkgreen', lty = 2) +
  geom_vline(xintercept = median(cv_conv_burn$run),
             colour = 'darkblue', lty = 2) +
  annotate(geom = 'text', label = 'mean',
           x = mean(cv_conv_burn$run) + 0.2, y = 20,
           colour = 'darkgreen', angle = 90) +
  annotate(geom = 'text', label = 'median',
           x = median(cv_conv_burn$run) - 0.2, y = 20,
           colour = 'darkblue', angle = 90) +
  theme_minimal() +
  labs(y = 'count',
       x = 'number of runs until cv convergence')

# plot both
cv_conv_plot <- p1 / p2
```

3) Windowed variance / relative outer variance
Width of window: 10 runs
With $\epsilon_\omega^2 = 0.2$
```{r}
numruns <- numruns %>% 
  group_by(combination_number) %>% 
  mutate(windowed_var = slider::slide_dbl(burn_percentage,
                                          var, .before = 5,
                                          .after = 5),
         # if windowed_var = 0, set relative_outer_var to 0 also
         relative_outer_var = ifelse(windowed_var == 0,
                                     0,
                                     windowed_var / max(windowed_var)))

# compute at which point the relative outer variance settles below 0.2
omega_conv <- numruns %>%
  group_by(combination_number) %>%
  filter(abs(relative_outer_var) < 0.2,
         run != 1) %>%
  slice(which.min(run))

# plot
p1 <- numruns %>%
  ggplot(mapping = aes(x = run, y = relative_outer_var,
                       group = combination_number)) +
  geom_line(colour = 'lightgrey') +
  geom_hline(yintercept = 0.2, colour = 'darkgreen') +
  labs(y = 'relative outer variance burn percentage') +
  theme_minimal()

p2 <- omega_conv %>%
  ggplot(mapping = aes(x = run)) +
  geom_histogram(binwidth = 1) +
  geom_vline(xintercept = mean(omega_conv$run),
             colour = 'darkgreen', lty = 2) +
  geom_vline(xintercept = median(omega_conv$run),
             colour = 'darkblue', lty = 2) +
  annotate(geom = 'text', label = 'mean',
           x = mean(omega_conv$run) + 1, y = 20,
           colour = 'darkgreen', angle = 90) +
  annotate(geom = 'text', label = 'median',
           x = median(omega_conv$run) - 1, y = 20,
           colour = 'darkblue', angle = 90) +
  theme_minimal() +
  labs(y = 'count',
       x = 'number of runs until relative outer variance convergence')

# plot both
outer_var_conv_plot <- p1 / p2
```

Inspect all results:
```{r}
# mean convergence
mean_conv_plot
ggsave('figures/fire_mean_convergence.pdf')

# coefficient of variation
cv_conv_plot
ggsave('figures/fire_coefficientofvariation.pdf')

# windowed variance
outer_var_conv_plot
ggsave('figures/fire_windowed_variance.pdf')
```

The mean convergence and coefficient of variation suggest 2 - 3 runs only. However, the relative outer variance suggests 20 runs, with some parameters even requiring as many as 40.

# Sampling parameter space
Because this model is not very large, I can simply sample the entire parameter space.
Set up the model and run 5 times for each density, record the percentage of trees burned at every tick:
```{python}
densities = list(range(1,100))
directions = [4, 8]
parameter_space_df = pd.DataFrame(columns = ['density', 'burn_percentage', 'directions', 'ticks'])

for y in directions:
  direction = str(y)
  netlogo.command(''.join(['set directions ', direction]))
  for d in densities:
    # density
    density = str(d)
    # set density
    netlogo.command(''.join(['set density ', density]))
    # set a random seed
    netlogo.command('clean')
    # repeat the execution of the model 10 times to account
    # for stochasticity
    for x in range(1,6):
      # set up
      netlogo.command('setup')
      initial = netlogo.report('initial-trees')
      # execute 1 tick at a time, until everything is burned up
      turtles = netlogo.report('count turtles')
      while turtles != 0:
        netlogo.command('go')
        # record burn percentage
        burned = netlogo.report('burned-trees')
        perc_burn = burned / initial * 100
        ticks = netlogo.report('ticks')
        # store in data frame
        parameter_space_df = parameter_space_df.append({'density': density, 'burn_percentage': perc_burn, 'directions': direction, 'ticks': ticks}, ignore_index = True)
        # check turtles
        turtles = netlogo.report('count turtles')
        
# save data as a csv file.
parameter_space_df.to_csv("C:/Users/Femke Keij/OneDrive/Thesis/R projects/LU thesis GitHub/data/raw/fire_output2.csv")
```

Close the model:
```{python}
netlogo.kill_workspace()
```

Stop running the python environment:
```{python}
quit
```

# Summarise repeated runs
Clear working directory:
```{r}
rm(list = ls(all = TRUE))
```

Import csv output format for the model as tidy data. We ignore the first column because it contains the indices.
```{r}
ws_output <- read_csv("data/raw/wolfsheep_output.csv",
                        col_names = TRUE)[, - 1]
```

Look at the distribution of the outputs within 1 parameter combination. If this is approximately normal (or concentrated around a single mean), we can summarise the data into a single value (the mean).
```{r}
# randomly sample 10 parameter combinations to look at
inspect <- sample(0:39, size = 10, replace = FALSE)

# correct tick numbers
ws_output$ticks <- rep(1:500, 600)

# for the random combination numbers to look at
for(i in inspect){
  p <- ws_output %>%
    # retrieve number and ticks 50, 100, and 150
    filter(combination_number == i,
          ticks %in% c(50, 100, 150)) %>%
    # pivot longer to shorten plot code
    pivot_longer(cols = c('sheep', 'wolves', 'grass'),
                 names_to = 'turtles',
                 values_to = 'values') %>%
    # make histograms with turtle / patch values
    ggplot(mapping = aes(x = values)) +
    geom_histogram(binwidth = 100) +
    # split for sheep, wolves, grass, and ticks
    facet_grid(turtles ~ ticks) +
    theme_minimal() +
    theme(panel.border = element_rect(fill = NA, colour = 'lightgrey')) +
    labs(caption = paste0('combination number ', as.character(i)))
  
  print(p)
}
```
Combination number 17 shows some dispersion. Otherwise, most runs seem to agree on the output and it seems reasonable to summarise using means.

Manipulate data so that there is 1 outcome for each parameter combination:
The outcomes for each parameter set are summarized by their mean sheep, wolves, and grass at every tick of the simulation.
```{r}
all_comb <- read_csv('data/processed/wolfsheep_parameterspace.csv')

# group the data by parameter combination and ticks
ws_summary <- ws_output %>%
  group_by(combination_number, ticks) %>%
  # take the mean wolf, sheep, and grass numbers over the 15 runs for each density
  summarise(mean_sheep = mean(sheep),
            mean_wolves = mean(wolves),
            mean_grass = mean(grass),
            .groups = 'keep')

# add the initial parameters back in
ws_summary$initial_number_sheep <- numeric(nrow(ws_summary))
ws_summary$initial_number_wolves <- numeric(nrow(ws_summary))
ws_summary$sheep_gain_from_food <- numeric(nrow(ws_summary))
ws_summary$wolves_gain_from_food <- numeric(nrow(ws_summary))
ws_summary$sheep_reproduce <- numeric(nrow(ws_summary))
ws_summary$wolves_reproduce <- numeric(nrow(ws_summary))
ws_summary$grass_regrowth <- numeric(nrow(ws_summary))

# change the combination numbers from 0 - 39 to 1 - 40
ws_summary$combination_number <- ws_summary$combination_number + 1

for(i in 1:nrow(ws_summary)){
  comb_numb <- ws_summary$combination_number[i]
  ws_summary$initial_number_sheep[i] <- as.numeric(all_comb[comb_numb, 1])
  ws_summary$initial_number_wolves[i] <- as.numeric(all_comb[comb_numb, 2])
  ws_summary$sheep_gain_from_food[i] <- as.numeric(all_comb[comb_numb, 3])
  ws_summary$wolves_gain_from_food[i] <- as.numeric(all_comb[comb_numb, 4])
  ws_summary$sheep_reproduce[i] <- as.numeric(all_comb[comb_numb, 5])
  ws_summary$wolves_reproduce[i] <- as.numeric(all_comb[comb_numb, 6])
  ws_summary$grass_regrowth[i] <- as.numeric(all_comb[comb_numb, 7])
}
  
ws_summary %>%
  write_csv(file = 'data/processed/wolfsheep_summary.csv')
```

# Sensitivity analysis
Using Morris' screening
```{r}
mo <- morris(model = NULL, factors = 3,
             r = 4, design = list(type = 'oat', levels = 5,
                                  grid.jump = 3),
             binf = c(1, 0, 0),
             bsup = c(99, 1, 1))

samples <- as_tibble(mo$X)

samples <- samples %>%
  mutate(directions = ifelse(X2 > X3, 4, 8)) %>%
  select(c('X1', 'directions')) %>%
  distinct(X1, directions)
```
Now run the model with those parameter combinations.

Local sensitivity:
Vary density between 20 and 80% while keeping directions to 4. Vary directions from 4 to 8 while keeping density at 50%.

```{python}
densities = [20, 30, 40, 50, 60, 70, 80]

# data frame to store results
sensitivity_results = pd.DataFrame(columns = ['density', 'directions', 'burn_percentage'])

# first for the densities
netlogo.command('set directions 4')

for i in densities:
  density = str(i)
  netlogo.command(''.join(['set density ', density]))
  # place to store intermediate results
  sensitivity_temp = list()
  for j in range(0, 10):
    # set up
    netlogo.command('setup')
    # execute for 10,000 ticks or until the model is done
    netlogo.command('repeat 10000 [go]')
    # record number of burned trees and number of initial trees
    initial = netlogo.report('initial-trees')
    burned = netlogo.report('burned-trees')
    # calculate % burned
    perc_burn = burned / initial * 100
    # store
    sensitivity_temp.append(perc_burn, ignore.index = True)
  # store average burn percentage as well as density and directions
  mean_burn = statistics.mean(sensitivity_temp)
  sensitivity_results = sensitivity_results.append({'perc_burn': mean_burn, 'density': i, 'directions': 4})


# then for the directions
netlogo.command('set density 50')
directions = [4, 8]

for i in directions:
  direction = str(i)
  netlogo.command(''.join(['set directions ', direction]))
  # place to store intermediate results
  sensitivity_temp = list()
  for j in range(0, 10):
    # set up
    netlogo.command('setup')
    # execute for 10,000 ticks or until the model is done
    netlogo.command('repeat 10000 [go]')
    # record number of burned trees and number of initial trees
    initial = netlogo.report('initial-trees')
    burned = netlogo.report('burned-trees')
    # calculate % burned
    perc_burn = burned / initial * 100
    # store
    sensitivity_temp = sensitivity_temp.append(perc_burn)
  # store average burn percentage as well as density and directions
  mean_burn = statistics.mean(sensitivity_temp)
  sensitivity_results = sensitivity_results.append({'perc_burn': mean_burn, 'density': 50, 'directions': i})
  
  
sensitivity_results
```

# Sampling parameter space

# Split into training and test data
Clear working directory:
```{r}
rm(list = ls(all = TRUE)) 
```

Import csv output format for the fire model as tidy data. We ignore the first column because it contains the indices.
```{r}
fire_output <- read_csv("data/raw/fire_output2.csv",
                        col_names = TRUE,
                        col_select = 2:5)
```

Manipulate data so that there is 1 outcome for each density:
The outcomes for each density are summarised by their mean burn percentage, and the minimum and maximum burn percentage
```{r}
# group the data by tree density
fire_output %>%
  group_by(density, directions) %>%
# take the mean burn % over the 10 runs for each density
# register the minimum and maximum burn % for each density
  summarise(burned = mean(burn_percentage), 
            max = max(burn_percentage),
            min = min(burn_percentage),
            .groups = 'keep') ->
# assign to new data frame
  fire_summary

fire_summary %>%
  write_csv(file = 'data/processed/fire_summary.csv')
```

Set up the training and test data:
I'm using a 80 - 20 split.
```{r}
set.seed(42)

ind <- sample(1:nrow(fire_output),
              size = nrow(fire_output) * 0.2,
              replace = FALSE)

fire_output %>%
  slice(ind) %>%
  write_csv(file = 'data/processed/fire_test.csv')

fire_output %>%
  slice(-ind) %>%
  write_csv(file = 'data/processed/fire_train.csv')
```