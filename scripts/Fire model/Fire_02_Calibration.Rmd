---
title: "Reading in NetLogo output files"
author: "Femke Keij S2647168"
date: '2022-07-14'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Clear working directory & set random seed:
```{r}
rm(list = ls(all = TRUE))

set.seed(42)
```

Packages used:
##########################
shouldn't these be in the source files?
I think here I might need to put 'requires'
##########################
```{r}
# for importing / working with tidy data
library(tidyverse)

# for arranging the ggplots
library(patchwork)

# to calculate error measures
library(modelr)

# for Latin hypercube sampling
library(lhs)

# for piecewise linear regression
library(segmented)

# for splines
library(splines)

# for random forest
library(randomForest)

# for elastic net
```

Source files for functions:
```{r}
# functions for error measures
source("scripts/Methods/error-measures-functions.R")

# functions for latin hypercube sampling
source("scripts/Methods/latin-hypercube-sampling-functions.R")

# functions for linear regression
# includes simple multiple linear regression and
    # piecewise linear regression
source("scripts/Methods/linear-regression-functions.R")

# functions for random forest
source("scripts/Methods/random-forest-functions.R")
```

Read in the data:
```{r}
# complete data set
fire_output <- read_csv(
  'data/raw/fire_output.csv')
fire_output$directions <- as.factor(fire_output$directions)

# training data
fire_train <- read_csv(
  'data/processed/fire_train.csv')
fire_train$directions <- as.factor(fire_train$directions)

# test data
fire_test <- read_csv(
  'data/processed/fire_test.csv')
fire_test$directions <- as.factor(fire_test$directions)
```

Random samples:
5, 10, 20, 30, 40, 50, 60, 70, and 80 $%$ of the total parameter space.
```{r}
N <- nrow(fire_output)
# range of (training) sample sizes to work with
n <- c(0.05 * N, 0.1 * N, 0.2 * N, 0.3 * N, 0.4 * N, 0.5 * N,
       0.6 * N, 0.7 * N, nrow(fire_train))
```

# Latin hypercube sampling: density & direction
Set up the Latin hypercube sample(s).
```{r}
# Function for Latin hypercube sampling using the ranges of each of the variables
LatinHypercubeSampling <- function(n, k = 2){
  # n: number of samples to draw
  # k: number of variables over which to sample
  
  # LHS with n samples over k variables
  A <- randomLHS(n = n, k = k)
  
  # transform samples to represent the parameter space
  # create empty matrix to hold parameter space samples
  B <- matrix(nrow = nrow(A), ncol = ncol(A))
  # round uniform distribution between 1 and 99 for 'tree density'
  B[,1] <- round(qunif(A[,1], min = 1, max = 99), digits = 0)
  # binomial distribution for '4 vs. 8 directions'
  B[,2] <- qbinom(p = A[,2], size = 1, prob = 0.5)
  B[, 2] <- B[, 2] * 4 + 4
  
  return(B)
}

# create an empty list to hold each of the LHSs
samples <- list()
# loop through the sample sizes and obtain LHS for each
for(i in 1:length(n)){
  samples[[i]] <- LatinHypercubeSampling(n = n[i], k = 3)
}
```

Now I take a random instance of the the training data for each of the LHS samples obtained above.
```{r}
for(i in 1:9){
  for(j in 1:n[i]){
    density <- samples[[i]][j, 1]
    direction <- samples[[i]][j, 2]
    ind_true <- which(fire_train$density == density & 
                   fire_train$directions == direction)
    ind_use <- sample(ind_true, size = 1)
    burn_perc <- fire_train$burn_percentage[ind_use]
    samples[[i]][j, 3] <- burn_perc
  }
}
```

Use these to estimate the test data:
```{r}
length_tib <- sum(nrow(fire_test) * length(n))
lhs_results <- tibble(direction_true = numeric(length_tib),
                      direction_pred = numeric(length_tib),
                      density_true = numeric(length_tib),
                      density_pred = numeric(length_tib),
                      burn_true = numeric(length_tib),
                      burn_pred = numeric(length_tib),
                      n = numeric(length_tib))
count <- 1

for(j in 1:9){
  for(k in 1:nrow(fire_test)){
  # take a burn percentage
  burn_match <- fire_test$burn_percentage[k]
  # see which instance of the LHS has the smallest distance
  # to that burn percentage
  diffs <- abs(samples[[j]][,3] - burn_match)
  ind <- which(diffs == min(diffs))
  # in case there are multiple instances that match
  if(length(ind) != 1){
    ind <- sample(ind, size = 1)
  }
  # retrieve the density and direction that generated
  # that minimum difference
  burn_perc <- samples[[j]][ind, 3]
  direction <- samples[[j]][ind, 2]
  density <- samples[[j]][ind, 1]
  # store the density, directions, and burn_percentages in a new dataframe
  lhs_results$direction_true[count] <- as.numeric(fire_test$directions[k])
  lhs_results$direction_pred[count] <- direction
  lhs_results$density_true[count] <- fire_test$density[k]
  lhs_results$density_pred[count] <- density
  lhs_results$burn_true[count] <- fire_test$burn_percentage[k]
  lhs_results$burn_pred[count] <- burn_perc
  lhs_results$n[count] <- n[j]
  count <- count + 1
  }
}

lhs_results %>%
  mutate(direction_true = replace(direction_true, direction_true == 1, 4),
         direction_true = replace(direction_true, direction_true == 2, 8)) -> lhs_results
```

Calculate the errors for the LHS.
Check in how many cases the true and predicted parameters correspond:
- both density & directions are correct
- density is correct
- direction is correct
```{r}
lhs_results %>%
  group_by(n) %>%
  summarise(perc_corect = sum(direction_true == direction_pred & density_true == density_pred) / nrow(fire_test),
            perc_density_correct = sum(density_true == density_pred) / nrow(fire_test),
            prec_direction_correct = sum(direction_true == direction_pred) / nrow(fire_test))
```
Make some bar charts out of this.

In how many cases is the density within the correct 10% interval?
```{r}
lhs_results %>%
  mutate(category_true = cut(density_true, breaks = c(-Inf, 10, 20, 30, 40, 50, 60, 70, 80, 90, Inf), labels = c('0 - 10', '11 - 20', '21 - 30', '31 - 40', '41 - 50', '51 - 60', '61 - 70', '71 - 80', '81 - 90', '91 - 99')),
         category_pred = cut(density_pred, breaks = c(-Inf, 10, 20, 30, 40, 50, 60, 70, 80, 90, Inf), labels = c('0 - 10', '11 - 20', '21 - 30', '31 - 40', '41 - 50', '51 - 60', '61 - 70', '71 - 80', '81 - 90', '91 - 99'))) -> lhs_results
```

```{r}
lhs_results %>%
  group_by(n) %>%
  summarise(perc_corect = sum(category_true == category_pred) / nrow(fire_test))
```

RMSE & NRMSE of true vs. predicted density:
```{r}
lhs_results %>%
  group_by(n) %>%
  summarise(RMSE = sqrt(sum((density_pred - density_true)^2)) / nrow(fire_test),
            NRMSE = (sqrt(sum((density_pred - density_true)^2)) / nrow(fire_test)) / sd(density_true))
```

Point prediction performance:
```{r}
# means of the density parameter in each of the training
# data sets
means <- numeric(9)
for(i in 1:9){
  means[i] <- mean(samples[[i]][,1])
}

# calculate point prediction performance
merge(lhs_results, as_tibble(cbind(n, means)), by = c('n')) %>%
  group_by(n) %>%
  summarise(point_pred_performance = 1 - sum(sqrt((density_pred - density_true)^2)) / sum(sqrt((density_true - means)^2)))
```

# Latin Hypercube Sampling: density, direction, & ticks


# LHS plots
Predicted vs. true (only for largest sample size for now):
```{r}
lhs_results$n <- as.factor(lhs_results$n)

lhs_results %>%
  filter(n == 1584) %>%
  ggplot(mapping = aes(x = density_pred, y = density_true,
                       colour = direction_true)) +
  geom_point()
```

# Regression

## Linear regression
Use the functions from the **Linear regression functions.R** file to get the errors for different training sample sizes
```{r}
linear_reg_RMSE <- rep(NA, length(n))
linear_reg_performance <- list()

for(i in 1:length(n)){
  fit <- LinearRegFitting(n = n[i],
                   training = fire_train,
                   formula = 'burn_percentage ~ density + direction')
  
  linear_reg_RMSE[i] <- LinearRegError(fit = fit,
                                           test = fire_test,
                                           y = 'burn_percentage')
  
  linear_reg_performance[[i]] <- CalcPerformance(fit = fit,
                                                 training = fire_train,
                                                 test = fire_test)
}
```

Plot the errors
```{r}
as.data.frame(cbind(n, linear_reg_RMSE)) %>%
  ggplot(aes(x = n, y = linear_reg_RMSE)) +
  geom_point() +
  labs(x = "training sample size", y = "RMSE between model outcome in test data and predicted outcome") -> p1


linear_reg_performance <- cbind(n,
                                matrix(unlist(linear_reg_performance),
                                       nrow = length(n),
                                       ncol = 2,
                                       byrow = TRUE))
colnames(linear_reg_performance) <- c("n", "density", "direction")

as.data.frame(linear_reg_performance) %>%
  ggplot() +
  geom_point(aes(x = n, y = density), colour = 'green') +
  geom_point(aes(x = n, y = direction), colour = 'red') +
  labs(x = "training sample size", y = "performance") -> p2

linear_reg_error_plot <- p1 + p2 + 
  plot_layout(ncol = 1) +
  plot_annotation(title = "Error progression in simple linear regression for different sample sizes",
                  subtitle = "green points for density, red points for direction")

linear_reg_error_plot
```

### Illustrate the errors for simple linear regression
```{r}
# use functions from `Linear regression functions.R` file
# create plot for a number of sample sizes
p10 <- LinearRegPlottingFire(n = 10, training = fire_train)
p50 <- LinearRegPlottingFire(n = 50, training = fire_train)
p100 <- LinearRegPlottingFire(n = 100, training = fire_train)
p500 <- LinearRegPlottingFire(n = 500, training = fire_train)
p1000 <- LinearRegPlottingFire(n = 100, training = fire_train)
pfull <- LinearRegPlottingFire(n = nrow(fire_train),
                          training = fire_train)

# combine plots
linear_reg_fit_plot <- p10 + p50 + p100 + p500 + p1000 + pfull +
  # split into 2 columns
  plot_layout(ncol = 2) +
  # title & caption
  plot_annotation(title = "Mutliple linear regression fit on the Fire model for different sample sizes",
                  subtitle = "Regression fit on % burned trees ~ tree density + direction of fire spread \n")

# display plot
linear_reg_fit_plot

# SAVE PLOT

```

### Prediction intervals
Use resampling bootstrapping to obtain the prediction intervals:

200 bootstrap data sets are produced and the same linear regression is run on each one. from each regression i, their prediction beta_i S(theta*) are collected and one standardized residual e is sampled (a residual divided by the square root of one minus the hat value associated with that residual). this produces a set of 200 beta_i S(theta*) + e_i. The 95% prediction interval is then defined by 2.5 and 97.5 percentile of this set.


```{r}
# create 200 bootstrap intervals out of the training data
bootstrap_samples <- list()
for(i in 1:200){
  bootstrap_sample <- fire_train[sample(1:nrow(fire_train),
                                        size = nrow(fire_train),
                                        replace = TRUE), ]
  bootstrap_lm_fit <- lm(formula = burn_percentage ~ density + direction,
                         data = bootstrap_sample)
  density_prediction <- predict(bootstrap_lm_fit,
                                newdata = fire_test,
                                type = "terms")[1]
  res <- abs(density_prediction - fire_test[, 2])
  hat <- density_prediction * 
  hat <- hatvalues(bootstrap_lm_fit)
  st_res <- res / sqrt(1 - hat)
}
```

## Piecewise linear regression
Calculate the piecewise linear regression fit for each of the directions and each of the sample sizes
```{r}
# create data frame to store the RMSEs for each sample size and direction
piecewise_reg_errors <- bind_cols(rep(n, 2),
                                  c(rep(4, length(n)),
                                         rep(8, length(n))),
                                  rep(NA, 2 * length(n)))
colnames(piecewise_reg_errors) <- c("N", "direction", "RMSE")

for(i in 2:length(n)){
  fit4 <- PiecewiseRegFitting(n = n[i],
                             training = filter(fire_train,
                                               direction == "4"),
                             formula = "burn_percentage ~ density")
  fit8 <- PiecewiseRegFitting(n = n[i],
                              training = filter(fire_train,
                                                direction == "8"),
                              formula = "burn_percentage ~ density")
  
  piecewise_reg_errors[i] <- 
    rmse(fit4,
         data = filter(fire_test, direction == "4"))
  piecewise_reg_errors[i + length(n)] <-
    rmse(fit8, 
         data = filter(fire_test, direction == "8"))
  #piecewise_reg_errors[i] <- 
   # PiecewiseRegError(fit = fit4, 
    #                  test = filter(fire_test, direction == "4"),
     #                 y = 'burn_percentage')
  #piecewise_reg_errors[i + length(n)] <- 
   # PiecewiseRegError(fit = fit8,
    #                  test = filter(fire_test, direction == "8"),
     #                 y = 'burn_percentage')
}
```

```{r}
piecewise_fit_4 <- lm(burn_percentage ~ density,
                      data = filter(fire_train, direction == "4"))

segmented_fit_4 <- segmented(piecewise_fit_4, 
                           seg.Z = ~ density,
                           psi = 59)
```

Plot the fits
```{r}
newdat_4 <- as.data.frame(cbind(fire_four_output$density,
                 broken.line(segmented_fit_4)$fit))
newdat_8 <- as.data.frame(cbind(fire_eight_output$density,
                 broken.line(segmented_fit_8)$fit))

ggplot() +
  geom_point(data = fire_train, aes(x = density, y = burn_percentage,
                                    color = direction)) +
  geom_line(data = newdat_4, aes(x = newdat_4[, 1], y = newdat_4[, 2]),
            colour = 'red') +
  geom_line(data = newdat_8, aes(x = newdat_8[, 1], y = newdat_8[, 2]),
            colour = 'blue') +
  labs(x = "density", y = "% burned trees at last tick")
```

Calculate the error (separately for each direction)
```{r}
# predict values in the test set, using the segmented fit
predict_piecewise_4 <- predict.segmented(segmented_fit_4,
                                         newdat = filter(fire_test,
                                                         direction == "4"))
predict_piecewise_8 <- predict.segmented(segmented_fit_8,
                                         newdat = filter(fire_test,
                                                         direction == "8"))

piecewise_rmse_4 <- sqrt(mean((filter(fire_test, direction == "4")$burn_percentage - predict_piecewise_4)^2))

piecewise_rmse_8 <- sqrt(mean((filter(fire_test, direction == "8")$burn_percentage - predict_piecewise_8)^2))
```

## Splines
### Cubic spline
```{r}

```
### Smoothing spline
```{r}

```

## Elastic net regression
Fit the elastic net (training using 5-fold cross-validation):
```{r}

```

# Neural networks

# Random forest

## Random forest
Fit the random forest for different sample sizes
```{r}
random_forest_errors <- matrix(NA, ncol = 2, nrow = length(n))
random_forest_errors[, 1] <- n
colnames(random_forest_errors) <- c("N", "RMSE")

for(i in 1:length(n)){
  # fit the random forest
  fit <- RandomForestFitting(n = n[i],
                             training = fire_train,
                             formula = "burn_percentage ~ density + direction")
  random_forest_errors[i, 2] <- RandomForestError(fit = fit,
                                               test = fire_test,
                                               y = 'burn_percentage')
}
```

Repeat each random forest 10 times to take out some of the variability:
```{r}
random_forest_errors_multiple <- matrix(NA,
                                        ncol = 2,
                                        nrow = 10 * length(n))
random_forest_errors_multiple[, 1] <- rep(n, 10)
colnames(random_forest_errors_multiple) <- c("N", "RMSE")

for(i in 1:nrow(random_forest_errors_multiple)){
  # fit the random forest
  fit <- RandomForestFitting(n = random_forest_errors_multiple[i, 1],
                             training = fire_train,
                             formula = "burn_percentage ~ density + direction")
  random_forest_errors_multiple[i, 2] <- RandomForestError(fit = fit,
                                                           test = fire_test,
                                                           y = "burn_percentage")
}
```

Show the error progression for different sample sizes
```{r}
as.data.frame(random_forest_errors_multiple) %>%
  group_by(N) %>%
  summarise(error = mean(RMSE),
            max = max(RMSE),
            min = min(RMSE)) %>%
  ggplot(aes(x = N, y = error)) +
  labs(x = "sample size", y = "mean RMSE over 10 random forests",
       title = "Root Mean Square Error in random forest",
       caption = "Blue dots are RMSE for a single random forest \n
       Green dots are mean RMSE over 10 different random forests \n
       Grey bands indicate minimum and maximum RMSE within the 10 random forests") +
  geom_point(colour = "dark green") +
  geom_point(data = as.data.frame(random_forest_errors),
             aes(x = N, y = RMSE), colour = "blue") +
  geom_ribbon(aes(ymin = min, ymax = max), alpha = 0.3)
```

Show what a random forest might look like for a single run with the full training sample
```{r}
random_forest <- randomForest(burn_percentage ~ density + direction,
                              data = fire_train)

plot(random_forest)

# plot the tree itself
```

## Bagging
Fit the bagging for different sample sizes
```{r}
bagging_errors <- matrix(NA, ncol = 2, nrow = length(n))
bagging_errors[, 1] <- n
colnames(bagging_errors) <- c("N", "RMSE")

for(i in 1:length(n)){
  # fit the bagging
  fit <- BaggingFitting(
    n = n[i],
    training = fire_train,
    formula = "burn_percentage ~ density + direction",
    p = 2)
  # calculate the bagging errors
  bagging_errors[i, 2] <- BaggingError(fit = fit,
                                       test = fire_test,
                                       y = 'burn_percentage')
}
```

Plot the bagging errors
```{r}
as.data.frame(bagging_errors) %>%
  ggplot(aes(x = N, y = RMSE)) +
  geom_point()
```

## Boosting

# Approximate Bayesian Computation (ABC)

# Genetic algorithms
